{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b0ae32-d608-4464-b5a2-93f3222be82b",
   "metadata": {},
   "source": [
    "# Defining and Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77933bed-341f-4546-a82e-c4f23bd605f9",
   "metadata": {},
   "source": [
    "What we will learn:\n",
    "- How to initialize a NN\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Optimization of the network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243891b-5b8c-4d77-9b01-251e9c07c6ff",
   "metadata": {},
   "source": [
    "## Pytorch: <code>nn</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae42ba-4062-4422-b84f-f43d7ab79537",
   "metadata": {},
   "source": [
    "The <code>nn</code> package defines a set of Modules (i.e. neural networks layers).\n",
    "\n",
    "Each module receive an input and produces an output.\n",
    "\n",
    "The <code>nn</code> package also defines losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d20409-f40d-4c15-be65-d1e445b8b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34795d-98da-4f49-8805-5c9cf7eed948",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Create a model that approximate the $sin(x)$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928f2e25-8f2b-425e-84dd-ce1890bd6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "859097be-808f-4736-bb34-4c350d3d206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -3.1416,   9.8696, -31.0063],\n",
      "        [ -3.1384,   9.8499, -30.9133],\n",
      "        [ -3.1353,   9.8301, -30.8205],\n",
      "        ...,\n",
      "        [  3.1353,   9.8301,  30.8205],\n",
      "        [  3.1384,   9.8499,  30.9133],\n",
      "        [  3.1416,   9.8696,  31.0063]])\n",
      "torch.Size([2000, 3])\n"
     ]
    }
   ],
   "source": [
    "# For this example, the network will learn the Sin function using a Polynomial Approximation.\n",
    "# The output y is a function of (x, x^2, x^3), so\n",
    "# we can consider it as an output of a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "print(xx)\n",
    "print(xx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cd4b337-b68f-4829-b3f9-c8221f71a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "\n",
    "# Costruiamo l'oggetto contenente la rete neurale:\n",
    "model = torch.nn.Sequential(\n",
    "    # layer lineare\n",
    "    torch.nn.Linear(3, 1),\n",
    "    # serve solo per flattenizzare\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5526daa5-aedd-42d0-82dd-09af2fc842be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "\n",
    "# Mi creo un oggetto contenente la funzione di errore MSE:\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# Mi definisco un learning rate:\n",
    "learning_rate = 1e-3\n",
    "# Construct the Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters\n",
    "# which are members of the model.\n",
    "\n",
    "# Uso SDG:\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc98c0-fff2-4100-b076-f663c8d60830",
   "metadata": {},
   "source": [
    "### MSE Loss\n",
    "$$\\mathcal{L}_{MSE} = \\frac{1}{n}\\sum^n_{i=n}(y - \\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9a79672-c22e-47c9-9df3-74d8aaf09299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 0.2046881467103958\n",
      "399 0.1372782289981842\n",
      "599 0.09257517009973526\n",
      "799 0.06292480230331421\n",
      "999 0.043254755437374115\n",
      "1199 0.030203089118003845\n",
      "1399 0.021541040390729904\n",
      "1599 0.01579095609486103\n",
      "1799 0.011972922831773758\n",
      "1999 0.009437178261578083\n"
     ]
    }
   ],
   "source": [
    "# Ciclo sul training set\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y) # (y_pred - y).pow(2).mean()\n",
    "    \n",
    "    # Print loss every 200 epochs\n",
    "    if t % 200 == 199:\n",
    "        # per stampare il valore numerico della loss, usa loss.item()\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Alternative: zero the gradients of the model\n",
    "    # model.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward() \n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Alternative: Update the weights using gradient descent MANUALLY. Each parameter is a Tensor, so\n",
    "    # we can access its gradients.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5133d785-f599-4cbe-8933-c718e60ac656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = -0.029610617086291313 + 0.7630205154418945 x + 0.005108322948217392 x^2 + -0.07999949157238007 x^3\n"
     ]
    }
   ],
   "source": [
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "090ad36c-c238-4d24-bdfe-6e7fe4f0a426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5717e-03, 2.4701e-06, 3.8821e-09])\n",
      "-0.028411 0.001572\n"
     ]
    }
   ],
   "source": [
    "# The network has effectively learned something?\n",
    "idx = 1000\n",
    "print(xx[idx]) # x[500] = -pi/2\n",
    "print(\"%.6f %.6f\" % (model(xx)[idx].item(), torch.sin(x)[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bccae464-ceb2-47d7-977a-6abf37fe3ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'res'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# plot results\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mres\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_data, plot_data_np, plot_model, set_default\n\u001b[0;32m      4\u001b[0m set_default()\n\u001b[0;32m      6\u001b[0m yy \u001b[38;5;241m=\u001b[39m model(xx)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'res'"
     ]
    }
   ],
   "source": [
    "# plot results\n",
    "import matplotlib.pyplot as plt\n",
    "from res.plot_lib import plot_data, plot_data_np, plot_model, set_default\n",
    "set_default()\n",
    "\n",
    "yy = model(xx)\n",
    "\n",
    "plt.plot(x,y, label='Sin(x)')\n",
    "plt.plot(x,yy.detach().numpy(), label='model(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b9b67-7ecb-4d4e-adbe-9dca48d9bcca",
   "metadata": {},
   "source": [
    "## Custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b499600-b3e9-457f-ba9a-c933c96a0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinModel(nn.Module):\n",
    "    def __init__(self, in_dim = 3, out_dim = 1):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate all the layer of the NN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # self.model = nn.Sequential(\n",
    "        #     nn.Linear(in_dim, out_dim),\n",
    "        #     nn.Flatten(0, 1)\n",
    "        # )\n",
    "        \"\"\"\n",
    "        In alternative we could also define each layer individually\n",
    "        \"\"\"\n",
    "        # Linear Layer della rete:\n",
    "        self.l1 = nn.Linear(in_dim, out_dim)\n",
    "        # Layer che flattenizza:\n",
    "        self.flt = nn.Flatten(0, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        #return self.model(x)\n",
    "\n",
    "        # Sempre meglio lavorare sul singolo modulo della rete e non darlo in pasto a self.model()!!\n",
    "        x = self.l1(x)\n",
    "        # do stuff: potrei infatti dover fare delle operazioni sul risultato del layer lineare:\n",
    "        return self.flt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc6349f-ce73-4c63-8309-fa600a765b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SinModel(\n",
      "  (l1): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (flt): Flatten(start_dim=0, end_dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "model = SinModel()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dd2d81e-c1a3-4c46-83d4-c31ce20d6f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 0.21490709483623505\n",
      "399 0.1475391834974289\n",
      "599 0.10184041410684586\n",
      "799 0.07080710679292679\n",
      "999 0.0497097373008728\n",
      "1199 0.03535117581486702\n",
      "1399 0.02556794509291649\n",
      "1599 0.0188945010304451\n",
      "1799 0.014337141066789627\n",
      "1999 0.011221330612897873\n"
     ]
    }
   ],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 200 == 199:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95f34ff4-0a0e-403d-89ed-9e8f7b30e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5700,  2.4649, -3.8700])\n",
      "-0.853562 -1.000000\n"
     ]
    }
   ],
   "source": [
    "# The network has effectively learned something?\n",
    "idx = 500\n",
    "print(xx[idx]) # x[500] = -pi/2\n",
    "print(\"%.6f %.6f\" % (model(xx)[idx].item(), torch.sin(x)[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c8ef6-ebc0-412e-82f0-bf312154b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = model(xx)\n",
    "\n",
    "plt.plot(x,y, label='Sin(x)')\n",
    "plt.plot(x,yy.detach().numpy(), label='model(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b4585-23a6-4204-b98c-140aa41d0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex1: write a model (using custom modules) where the output y is a function of (x, x^2)\n",
    "# and it approximates the cosine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44b074cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 2])\n",
      "tensor([[-3.1416,  9.8696],\n",
      "        [-3.1384,  9.8499],\n",
      "        [-3.1353,  9.8301],\n",
      "        ...,\n",
      "        [ 3.1353,  9.8301],\n",
      "        [ 3.1384,  9.8499],\n",
      "        [ 3.1416,  9.8696]])\n",
      "torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "# Andiamo a creare per prima cosa il dataset:\n",
    "x_cosine = torch.linspace(-torch.pi, torch.pi, 2000)\n",
    "y_cosine = torch.cos(x_cosine)\n",
    "# Cambio il Tensor delle potenze perchè qui sto lavorando con un polinomio di secondo grado:\n",
    "power = torch.Tensor([1, 2])\n",
    "# Creo la struttura dati per il modello:\n",
    "xx_cosine = x_cosine.unsqueeze(-1).pow(power)\n",
    "print(xx_cosine.size())\n",
    "print(xx_cosine)\n",
    "print(y_cosine.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03d72410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CosineModule(\n",
      "  (l1): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (f): Flatten(start_dim=0, end_dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CosineModule(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # costruttore classe padre:\n",
    "        super().__init__()\n",
    "        # creazione del layer lineare:\n",
    "        self.l1 = nn.Linear(input_dim, output_dim)\n",
    "        self.f = nn.Flatten(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.l1(x)\n",
    "        return self.f(result)\n",
    "    \n",
    "# Istanziazione dell'oggetto:\n",
    "cosine_nn = CosineModule(2, 1)\n",
    "print(cosine_nn)\n",
    "# Istanzio la loss function:\n",
    "my_loss_function = nn.MSELoss()\n",
    "# Istanzio l'optimizer:\n",
    "my_optimizer = torch.optim.SGD(cosine_nn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45ae528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([2000])) that is different to the input size (torch.Size([2000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.643784761428833\n",
      "99 0.8743630051612854\n",
      "198 0.6672363877296448\n",
      "297 0.6009028553962708\n",
      "396 0.5742444396018982\n",
      "495 0.559681236743927\n",
      "594 0.549544095993042\n",
      "693 0.5415782332420349\n",
      "792 0.5350212454795837\n",
      "891 0.5295374989509583\n",
      "990 0.5249274969100952\n",
      "1089 0.5210455060005188\n",
      "1188 0.5177748799324036\n",
      "1287 0.5150187611579895\n",
      "1386 0.5126962065696716\n",
      "1485 0.5107388496398926\n",
      "1584 0.5090893507003784\n",
      "1683 0.5076992511749268\n",
      "1782 0.5065277218818665\n",
      "1881 0.5055404305458069\n",
      "1980 0.5047084093093872\n"
     ]
    }
   ],
   "source": [
    "# Addestramento della rete:\n",
    "for t in range(2000):\n",
    "    # calcolo la prediction:\n",
    "    y_pred = cosine_nn(xx_cosine)\n",
    "    # calcolo la loss e la stampo ogni 100 iterazioni:\n",
    "    loss = my_loss_function(y_pred, y_cosine)\n",
    "    if t%99 == 0:\n",
    "        print(t, loss.item())\n",
    "    # backward pass e aggiornamento dei pesi della rete:\n",
    "    my_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    my_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "961f0d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.088470 0.707940\n"
     ]
    }
   ],
   "source": [
    "# Vediamo se la rete ha appreso:\n",
    "idx = 750\n",
    "print(\"%.6f %.6f\" % (cosine_nn(xx_cosine)[idx].item(), torch.cos(x)[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611592dd-58ac-40d7-9a4b-0b92fe51bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex2: write a model (using custom modules) where the output y is a function of (x, x^2, x^3)\n",
    "# and it approximates the function -5 + 2*x + 3/4x^2 + 7*x^3\n",
    "# which values do you expect bias and weights should have after training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb58e934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x:  torch.Size([2000])\n",
      "Size of y:  torch.Size([2000])\n",
      "Size of xx:  torch.Size([2000, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creazione del dataset:\n",
    "x = torch.linspace(-1, +1, 2000) #usa un intervallino più ristretto per evitare loss troppo grandi\n",
    "y = (-5 + 2*x + (3/4)*x**2 + 7*x**3).to(torch.float32)\n",
    "p = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "print(\"Size of x: \", x.size())\n",
    "print(\"Size of y: \", y.size())\n",
    "print(\"Size of xx: \", xx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c3f838cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ApproximatorModel(\n",
      "  (l1): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (f): Flatten(start_dim=0, end_dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ApproximatorModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # chiamata al costruttore della classe padre:\n",
    "        super().__init__()\n",
    "        # creazione del primo layer lineare:\n",
    "        self.l1 = nn.Linear(input_dim, output_dim)\n",
    "        # creazione del layer di flattening:\n",
    "        self.f = nn.Flatten(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.l1(x)\n",
    "        return self.f(result)\n",
    "\n",
    "# Istanzio un oggetto del modello:\n",
    "approximator = ApproximatorModel(3, 1)\n",
    "# Istanzio la funzione di errore:\n",
    "my_loss_function = nn.MSELoss()\n",
    "# Istazio l'optimizer:\n",
    "optimizer = torch.optim.SGD(approximator.parameters(), lr=1e-3)\n",
    "\n",
    "print(approximator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8044b2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9663819074630737\n",
      "99 0.8944557905197144\n",
      "198 0.8332880139350891\n",
      "297 0.7810604572296143\n",
      "396 0.7362793684005737\n",
      "495 0.6977102160453796\n",
      "594 0.6643339991569519\n",
      "693 0.6353052854537964\n",
      "792 0.6099230051040649\n",
      "891 0.5876033902168274\n",
      "990 0.5678605437278748\n",
      "1089 0.5502913594245911\n",
      "1188 0.5345574617385864\n",
      "1287 0.5203779339790344\n",
      "1386 0.5075181126594543\n",
      "1485 0.4957819879055023\n",
      "1584 0.48500487208366394\n",
      "1683 0.47504961490631104\n",
      "1782 0.4658012092113495\n",
      "1881 0.4571630656719208\n",
      "1980 0.44905394315719604\n",
      "2079 0.44140613079071045\n",
      "2178 0.43416205048561096\n",
      "2277 0.42727360129356384\n",
      "2376 0.4206998944282532\n",
      "2475 0.41440650820732117\n",
      "2574 0.40836402773857117\n",
      "2673 0.4025477170944214\n",
      "2772 0.39693647623062134\n",
      "2871 0.3915121853351593\n",
      "2970 0.3862591087818146\n",
      "3069 0.38116395473480225\n",
      "3168 0.3762151300907135\n",
      "3267 0.3714025914669037\n",
      "3366 0.3667176365852356\n",
      "3465 0.3621525168418884\n",
      "3564 0.3577004373073578\n",
      "3663 0.3533552885055542\n",
      "3762 0.34911176562309265\n",
      "3861 0.3449651896953583\n",
      "3960 0.3409111797809601\n",
      "4059 0.33694562315940857\n",
      "4158 0.33306512236595154\n",
      "4257 0.32926636934280396\n",
      "4356 0.3255460858345032\n",
      "4455 0.3219020962715149\n",
      "4554 0.31833115220069885\n",
      "4653 0.3148309588432312\n",
      "4752 0.3113992512226105\n",
      "4851 0.30803418159484863\n",
      "4950 0.30473315715789795\n",
      "5049 0.3014945387840271\n",
      "5148 0.29831621050834656\n",
      "5247 0.2951972782611847\n",
      "5346 0.29213476181030273\n",
      "5445 0.28912830352783203\n",
      "5544 0.28617486357688904\n",
      "5643 0.2832748293876648\n",
      "5742 0.2804264426231384\n",
      "5841 0.2776254415512085\n",
      "5940 0.27487319707870483\n",
      "6039 0.27216827869415283\n",
      "6138 0.26950928568840027\n",
      "6237 0.2668948471546173\n",
      "6336 0.26432371139526367\n",
      "6435 0.26179584860801697\n",
      "6534 0.2593095600605011\n",
      "6633 0.2568630576133728\n",
      "6732 0.25445523858070374\n",
      "6831 0.2520863115787506\n",
      "6930 0.24975430965423584\n",
      "7029 0.24745872616767883\n",
      "7128 0.245198592543602\n",
      "7227 0.24297289550304413\n",
      "7326 0.24078115820884705\n",
      "7425 0.23862211406230927\n",
      "7524 0.23649519681930542\n",
      "7623 0.2343999296426773\n",
      "7722 0.23233507573604584\n",
      "7821 0.2302999198436737\n",
      "7920 0.22829391062259674\n",
      "8019 0.22631677985191345\n",
      "8118 0.22436761856079102\n",
      "8217 0.22244559228420258\n",
      "8316 0.22054937481880188\n",
      "8415 0.21868009865283966\n",
      "8514 0.2168358713388443\n",
      "8613 0.21501614153385162\n",
      "8712 0.2132209837436676\n",
      "8811 0.2114497572183609\n",
      "8910 0.2097010612487793\n",
      "9009 0.20797494053840637\n",
      "9108 0.206272155046463\n",
      "9207 0.20459014177322388\n",
      "9306 0.20292872190475464\n",
      "9405 0.201289564371109\n",
      "9504 0.19967015087604523\n",
      "9603 0.19806988537311554\n",
      "9702 0.19649016857147217\n",
      "9801 0.19492904841899872\n",
      "9900 0.1933862417936325\n",
      "9999 0.19186270236968994\n",
      "tensor([-0.4997,  0.2497, -0.1248])\n",
      "-7.182031 -6.685874\n"
     ]
    }
   ],
   "source": [
    "# Addestramento del modello:\n",
    "for t in range(10000):\n",
    "    # calcolo il predicted result:\n",
    "    y_pred = approximator(xx)\n",
    "    # calcolo la loss e la stampo ogni 100 iterazioni:\n",
    "    loss = my_loss_function(y_pred, y)\n",
    "    if t%99 == 0:\n",
    "        print(t, loss.item())\n",
    "    # backward pass + aggiornamento dei pesi:\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# La rete avrà imparato qualcosa?\n",
    "idx = 500\n",
    "print(xx[idx])\n",
    "print(\"%.6f %.6f\" % (approximator(xx)[idx].item(), y[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7604ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = -4.878225803375244 + 3.766690731048584 x + 0.4140462577342987 x^2 + 4.204785346984863 x^3\n"
     ]
    }
   ],
   "source": [
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = approximator.l1\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

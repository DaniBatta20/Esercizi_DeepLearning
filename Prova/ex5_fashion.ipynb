{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bde5a502b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiegazione\n",
    "In questo file ho provato a fare un piccolo esperimento per i fatti miei. Ho voluto lavorare sul Fashion Mnist (immagini a toni di grigio 28x28) è creare un modello alternativo di \n",
    "CNN capace di fare classificazione multi-classi.\n",
    "\n",
    "Notiamo che all'aumentare del numero di layer lineari (CLASSIFICATORE) aumenta anche il numero di epoche necessarie per l'addestramento. Le performance faranno inizialmente schifo perchè la rete tirerà completamente a caso, però dopo un po' inizia a migliorare notevolmente fino a raggiungere un overfitting se superi le 4 epoche. \n",
    "Si nota infatti che l'andamento della loss sul training è altalenante e che non si hanno miglioramenti sul test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparazione del modello: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.sgd\n",
    "\n",
    "\n",
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Chiamata al costruttore della classe base:\n",
    "        super(FashionCNN, self).__init__()\n",
    "        # Prima convoluzione\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=3,\n",
    "            kernel_size=7 ,\n",
    "            stride=1,\n",
    "            padding=3\n",
    "        )\n",
    "        # Max pooling:\n",
    "        self.pooling1 = nn.MaxPool2d(\n",
    "            kernel_size=2\n",
    "        )\n",
    "        # Seconda convoluzione\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=6,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2\n",
    "        )\n",
    "        # Max pooling:\n",
    "        self.pooling2 = nn.MaxPool2d(\n",
    "            kernel_size=2\n",
    "        )\n",
    "        # Linear layers:\n",
    "        self.linear1 = nn.Linear(in_features=6*7*7, out_features=100)\n",
    "        self.linear2 = nn.Linear(in_features=100, out_features=50)\n",
    "        self.linear3 = nn.Linear(in_features=50, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x è un'immagine a toni di grigio (1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = self.pooling1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pooling2(x)\n",
    "        # Ora devo flattinizzare x per poter lavorare sui layer lineari:\n",
    "        x = x.view(-1, 6*7*7)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Istanzio il modello, l'optimizer\n",
    "model = FashionCNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparazione del dataset Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 938\n"
     ]
    }
   ],
   "source": [
    "# Mi creo l'oggetto contenente il training dataset\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    '../data',          # path in cui salvo\n",
    "    download=True,     \n",
    "    train=True,         # prendo il training set\n",
    "    transform=transforms.Compose([\n",
    "        # No resize, sono già 28x28\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.268), (0.353))\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Ora mi creo il loader per il training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Oggetto contenente i dati del test set:\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    '../data',  #path\n",
    "    download=True,\n",
    "    train=False,    # prendo solo il test\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.268), (0.353))\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Creo un test_set loader:\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=1000, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(len(train_dataset), len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addestramento della rete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test function\n",
    "accuracy_list = []\n",
    "def train(epoch, model, optimizer, perm=None):\n",
    "    model.train()\n",
    "    # dataloader will iterate the dataset and return images (data)\n",
    "    # and labels (target)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device) #carico data e target sul device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, perm=None):\n",
    "    # metto il modello in evaluation(), così non avviene l'addestramento!!!\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        # qua anzichè definire una lista di test loss usa un accumulatore unico, perchè non dobbiamo ciclare sui batch come durante il tranining\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.293220\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.287865\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.260003\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.225650\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.148962\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.005921\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.723622\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.452485\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.279127\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.071867\n",
      "\n",
      "Test set: Average loss: 1.0393, Accuracy: 6415/10000 (64%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.146665\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.961202\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.002684\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.894456\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.945238\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.973942\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.800318\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.747620\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.885011\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.789820\n",
      "\n",
      "Test set: Average loss: 0.7919, Accuracy: 7123/10000 (71%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.847804\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.634794\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.749914\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.750846\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.790172\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.693039\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.771233\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.589937\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.720736\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.639614\n",
      "\n",
      "Test set: Average loss: 0.7311, Accuracy: 7238/10000 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "for epoch in range(0, 3):\n",
    "    train(epoch, model, optimizer)\n",
    "    test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
